{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dce7c0f",
   "metadata": {},
   "source": [
    "# HW1-3: Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5010e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "TORCH_CUDA_ARCH_LIST=\"8.6\"\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transformtransforms\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "Project_PATH = os.path.dirname(os.path.abspath('__file__'))\n",
    "outputs_dir = Project_PATH + '/'\n",
    "model_path = Project_PATH + '/save_models/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "device_default = torch.cuda.current_device()\n",
    "torch.cuda.device(device_default)\n",
    "print(torch.cuda.get_device_name(device_default))\n",
    "device = torch.device(\"cuda\")\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_MNIST_N(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_1, hidden_2, hidden_3, out_dim):\n",
    "        super(DNN_MNIST_N, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_dim, hidden_1),nn.BatchNorm1d(hidden_1),nn.ReLU(True))\n",
    "        self.layer2 = nn.Sequential(nn.Linear(hidden_1, hidden_2),nn.BatchNorm1d(hidden_2),nn.ReLU(True))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(hidden_2, hidden_3),nn.BatchNorm1d(hidden_3),nn.ReLU(True))\n",
    "        self.layer4 = nn.Sequential(nn.Linear(hidden_3, out_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x) \n",
    "        x = self.layer2(x)    \n",
    "        x = self.layer3(x)    \n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "    \n",
    "# device = torch.device(\"cuda\")\n",
    "# Model = DNN_MNIST_N(28*28,10,20,10,10).to(device)\n",
    "# Model.eval()\n",
    "# print('# of total parameters: ', sum(param.numel() for param in Model.parameters()))\n",
    "# summary(Model, input_size=(1,28*28))\n",
    "\n",
    "class DNN_MNIST_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN_MNIST_3, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(28*28, 256),nn.BatchNorm1d(256),nn.ReLU(True))\n",
    "        self.layer2 = nn.Sequential(nn.Linear(256, 128),nn.BatchNorm1d(128),nn.ReLU(True))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(128, 64),nn.BatchNorm1d(64),nn.ReLU(True))\n",
    "        self.layer4 = nn.Sequential(nn.Linear(64, 32),nn.BatchNorm1d(32),nn.ReLU(True))\n",
    "        self.layer5 = nn.Sequential(nn.Linear(32, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x) \n",
    "        x = self.layer2(x)    \n",
    "        x = self.layer3(x)    \n",
    "        x = self.layer4(x)    \n",
    "        x = self.layer5(x)    \n",
    "        return x\n",
    "    \n",
    "# device = torch.device(\"cuda\")\n",
    "# Model_DNN_MNIST_3 = DNN_MNIST_3().to(device)\n",
    "# summary(Model_DNN_MNIST_3, input_size=(1000,28*28))\n",
    "\n",
    "def standardization(x):\n",
    "    x = np.array(x)\n",
    "    x[np.isnan(x)] = 0\n",
    "    return (x-np.mean(x))/np.std(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "940c1fd1",
   "metadata": {},
   "source": [
    "## Can network fit random labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3766b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Define train function\n",
    "'''\n",
    "def train_shuffle(model_name,\n",
    "                Epochs = 100,\n",
    "                Batch  = 2000,\n",
    "                Data_workers = 0,\n",
    "                LR = 0.01):\n",
    "    '''\n",
    "    Initiate data\n",
    "    '''\n",
    "#     transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "#     trainset = torchvision.datasets.CIFAR10(root='./data/',train=True,download=True,transform=transform)\n",
    "#     testset = torchvision.datasets.CIFAR10(root='./data/',train=False,download=True,transform=transform)\n",
    "    trainset = torchvision.datasets.MNIST(root='./data/',train=True,download=True,transform=transforms.ToTensor())\n",
    "    testset = torchvision.datasets.MNIST(root='./data/',train=False,download=True,transform=transforms.ToTensor())\n",
    "    \n",
    "    # Shuffle labels\n",
    "    random.shuffle(trainset.train_labels)\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=Batch, shuffle=True, num_workers=Data_workers)\n",
    "    testloader  = DataLoader(testset,  batch_size=Batch, shuffle=True, num_workers=Data_workers)\n",
    "    print(trainset.classes)\n",
    "    print(trainset.data.shape)\n",
    "    print(testset.data.shape)\n",
    "    '''\n",
    "    Initiate model\n",
    "    '''\n",
    "    torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "    Model = model_name.to(device)\n",
    "    '''\n",
    "    loss & optimizer\n",
    "    '''\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(Model.parameters(), lr=LR, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.8)\n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    trainloss_list = []\n",
    "    testloss_list  = []\n",
    "    accuracy_list  = []\n",
    "    lr_list = []\n",
    "   \n",
    "    for epoch in range(Epochs):\n",
    "        Model.train()\n",
    "        train_loss = 0.0\n",
    "#         with tqdm(total=(len(trainset) - len(trainset) % Batch)) as t:\n",
    "#             t.set_description('epoch: {}/{}'.format(epoch+1, Epochs))\n",
    "        for i, data in enumerate(trainloader):\n",
    "            images, labels = data\n",
    "#             images = images.to(device)\n",
    "            images = (images.view(-1, 28*28)).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = Model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            total = (i+1)*Batch\n",
    "#             t.set_postfix(loss='{:.6f}'.format(train_loss))\n",
    "#             t.update(len(images))\n",
    "        '''\n",
    "        Evaluating\n",
    "        '''\n",
    "        Model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "#                 images = images.to(device)\n",
    "                images = (images.view(-1, 28*28)).to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = Model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, pred = torch.max(outputs.data, 1)\n",
    "                correct += (pred == labels).cpu().sum()\n",
    "                total += labels.size(0)\n",
    "            total = len(testloader.dataset)\n",
    "            accuracy = 100.0*correct/total\n",
    "        '''\n",
    "        Save loss\n",
    "        '''\n",
    "        scheduler.step()\n",
    "        lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        trainloss_list.append(train_loss)\n",
    "        testloss_list.append(test_loss)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('{}/{} Test set: Average loss: {:.4f}/{:.4f}, Accuracy: {}/{} ({:.2f}%) lr={}'.format(\n",
    "                epoch, Epochs, train_loss,test_loss, correct, total, accuracy, lr_list[-1]))\n",
    "\n",
    "    return [trainloss_list,\n",
    "            testloss_list,\n",
    "            accuracy_list,\n",
    "            lr_list]\n",
    "\n",
    "\n",
    "[trainloss_list,testloss_list,accuracy_list,lr_list] = train_shuffle(model_name=DNN_MNIST_3())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e032254",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(np.array(trainloss_list), label='train_loss')\n",
    "plt.plot(np.array(testloss_list)*6, label='test_loss')\n",
    "plt.xlabel('epoch',fontsize=20)\n",
    "plt.ylabel('loss',fontsize=20)\n",
    "plt.title('Loss',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(accuracy_list, label='accuracy')\n",
    "plt.xlabel('epoch',fontsize=20)\n",
    "plt.ylabel('accuracy',fontsize=20)\n",
    "plt.title('accuracy',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abb2e067",
   "metadata": {},
   "source": [
    "## Number of parameters v.s. Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_CIFAR10(model_name, Epochs=20, Batch=2000, Data_workers=0, LR=0.1):\n",
    "    '''\n",
    "    Load datasets\n",
    "    '''\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data/',train=True,download=True,transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data/',train=False,download=True,transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=Batch, shuffle=True, num_workers=Data_workers)\n",
    "    testloader  = DataLoader(testset,  batch_size=Batch, shuffle=True, num_workers=Data_workers)\n",
    "    print(trainset.classes)\n",
    "    print(trainset.data.shape)\n",
    "    print(testset.data.shape)\n",
    "    torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    Model = model_name.to(device)\n",
    "    num_param = sum(param.numel() for param in Model.parameters())\n",
    "    print(model_name)\n",
    "    print('# of total parameters: ', num_param)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(Model.parameters(), lr=LR, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 2, gamma = 0.8)\n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    trainloss_list = []\n",
    "    testloss_list  = []\n",
    "    train_acc_list  = []\n",
    "    test_acc_list = []\n",
    "    lr_list = []\n",
    "    for epoch in range(Epochs):\n",
    "        Model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            images, labels = data\n",
    "            images = (images.view(-1, 3*32*32)).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = Model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            train_correct += (pred == labels).cpu().sum()\n",
    "        train_total = len(trainloader.dataset)\n",
    "        train_acc = 100.0*train_correct/train_total\n",
    "        '''\n",
    "        Evaluating\n",
    "        '''\n",
    "        Model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images = (images.view(-1, 3*32*32)).to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = Model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, pred = torch.max(outputs.data, 1)\n",
    "                test_correct += (pred == labels).cpu().sum()\n",
    "            test_total = len(testloader.dataset)\n",
    "            test_acc = 100.0*test_correct/test_total\n",
    "\n",
    "        '''\n",
    "        Save loss\n",
    "        '''\n",
    "#         scheduler.step()\n",
    "        lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        trainloss_list.append(train_loss)\n",
    "        testloss_list.append(test_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('{}/{} Test set: Average loss: {:.4f}/{:.4f}, Accuracy: {}/{} ({:.2f}%)/({:.2f}%) lr={}'.format(\n",
    "                epoch, Epochs, train_loss,test_loss, train_correct,test_correct, train_acc,test_acc, lr_list[-1]))\n",
    "\n",
    "    return [trainloss_list, testloss_list,\n",
    "            train_acc_list, test_acc_list,\n",
    "            lr_list, num_param]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60171f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[trainloss_1,testloss_1,train_acc_1,test_acc_1,_,num_param_1] = train_CIFAR10(model_name=DNN_MNIST_N(3*32*32,1,2,1,10),Epochs=100)\n",
    "[trainloss_2,testloss_2,train_acc_2,test_acc_2,_,num_param_2] = train_CIFAR10(model_name=DNN_MNIST_N(3*32*32,2,4,2,10),Epochs=100)\n",
    "[trainloss_3,testloss_3,train_acc_3,test_acc_3,_,num_param_3] = train_CIFAR10(model_name=DNN_MNIST_N(3*32*32,3,6,3,10),Epochs=100)\n",
    "[trainloss_4,testloss_4,train_acc_4,test_acc_4,_,num_param_4] = train_CIFAR10(model_name=DNN_MNIST_N(3*32*32,5,10,5,10),Epochs=100)\n",
    "[trainloss_5,testloss_5,train_acc_5,test_acc_5,_,num_param_5] = train_CIFAR10(model_name=DNN_MNIST_N(3*32*32,10,20,10,10),Epochs=100)\n",
    "[trainloss_6,testloss_6,train_acc_6,test_acc_6,_,num_param_6] = train_CIFAR10(model_name=DNN_MNIST_N(3*32*32,20,40,20,10),Epochs=100)\n",
    "[trainloss_7,testloss_7,train_acc_7,test_acc_7,_,num_param_7] = train_CIFAR10(model_name=DNN_MNIST_N(3*32*32,50,100,50,10),Epochs=100)\n",
    "[trainloss_8,testloss_8,train_acc_8,test_acc_8,_,num_param_8] = train_CIFAR10(model_name=DNN_MNIST_N(3*32*32,100,200,100,10),Epochs=100)\n",
    "[trainloss_9,testloss_9,train_acc_9,test_acc_9,_,num_param_9] = train_CIFAR10(model_name=DNN_MNIST_N(3*32*32,200,400,200,10),Epochs=100)\n",
    "[trainloss_10,testloss_10,train_acc_10,test_acc_10,_,num_param_10] = train_CIFAR10(model_name=DNN_MNIST_N(3*32*32,500,1000,500,10),Epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cca24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_param_list = [num_param_1,num_param_2,num_param_3,num_param_4,num_param_5,num_param_6,num_param_7,num_param_8,num_param_9,num_param_10]\n",
    "\n",
    "trainloss_list = [trainloss_1,trainloss_2,trainloss_3,trainloss_4,trainloss_5,trainloss_6,trainloss_7,trainloss_8,trainloss_9,trainloss_10]\n",
    "trainloss = np.array(trainloss_list)[:,-1]\n",
    "testloss_list = [testloss_1,testloss_2,testloss_3,testloss_4,testloss_5,testloss_6,testloss_7,testloss_8,testloss_9,testloss_10]\n",
    "testloss = np.array(testloss_list)[:,-1]\n",
    "\n",
    "train_acc_list = [train_acc_1,train_acc_2,train_acc_3,train_acc_4,train_acc_5,train_acc_6,train_acc_7,train_acc_8,train_acc_9,train_acc_10]\n",
    "train_acc = np.array(train_acc_list)[:,-1]\n",
    "test_acc_list = [test_acc_1,test_acc_2,test_acc_3,test_acc_4,test_acc_5,test_acc_6,test_acc_7,test_acc_8,test_acc_9,test_acc_10]\n",
    "test_acc = np.array(test_acc_list)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plot loss & lr\n",
    "'''\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot((num_param_list), (trainloss), label='train_loss')\n",
    "plt.plot((num_param_list), (testloss*5), label='test_loss')\n",
    "plt.xlabel('num_param',fontsize=20)\n",
    "plt.ylabel('loss',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(num_param_list, train_acc, label='train_acc')\n",
    "plt.plot(num_param_list, test_acc, label='test_acc')\n",
    "plt.xlabel('num_param',fontsize=20)\n",
    "plt.ylabel('accuracy',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea49882e",
   "metadata": {},
   "source": [
    "## Flatness v.s. Generalization - part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa362ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define train function\n",
    "'''\n",
    "def train_MNIST(model_name,\n",
    "                Epochs = 20,\n",
    "                Batch  = 2000,\n",
    "                Data_workers = 0,\n",
    "                LR = 0.1):\n",
    "    '''\n",
    "    Initiate data\n",
    "    '''\n",
    "#     transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "#     trainset = torchvision.datasets.CIFAR10(root='./data/',train=True,download=True,transform=transform)\n",
    "#     testset = torchvision.datasets.CIFAR10(root='./data/',train=False,download=True,transform=transform)\n",
    "    trainset = torchvision.datasets.MNIST(root='./data/',train=True,download=True,transform=transforms.ToTensor())\n",
    "    testset = torchvision.datasets.MNIST(root='./data/',train=False,download=True,transform=transforms.ToTensor())\n",
    "    trainloader = DataLoader(trainset, batch_size=Batch, shuffle=True, num_workers=Data_workers)\n",
    "    testloader  = DataLoader(testset,  batch_size=Batch, shuffle=True, num_workers=Data_workers)\n",
    "    print(trainset.classes)\n",
    "    print(trainset.data.shape)\n",
    "    print(testset.data.shape)\n",
    "    '''\n",
    "    Initiate model\n",
    "    '''\n",
    "    torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "    Model = model_name.to(device)\n",
    "    '''\n",
    "    loss & optimizer\n",
    "    '''\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(Model.parameters(), lr=LR, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.8)\n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    trainloss_list = []\n",
    "    testloss_list  = []\n",
    "    accuracy_list  = []\n",
    "    lr_list = []\n",
    "    F = []\n",
    "   \n",
    "    for epoch in range(Epochs):\n",
    "        Model.train()\n",
    "        train_loss = 0.0\n",
    "#         with tqdm(total=(len(trainset) - len(trainset) % Batch)) as t:\n",
    "#             t.set_description('epoch: {}/{}'.format(epoch+1, Epochs))\n",
    "        for i, data in enumerate(trainloader):\n",
    "            images, labels = data\n",
    "#             images = images.to(device)\n",
    "            images = (images.view(-1, 28*28)).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = Model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "#             grads = torch.autograd.grad(outputs, images, retain_graph=True)\n",
    "#             F.append(grads)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            total = (i+1)*Batch\n",
    "#             t.set_postfix(loss='{:.6f}'.format(train_loss))\n",
    "#             t.update(len(images))\n",
    "        '''\n",
    "        Evaluating\n",
    "        '''\n",
    "        Model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "#                 images = images.to(device)\n",
    "                images = (images.view(-1, 28*28)).to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = Model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, pred = torch.max(outputs.data, 1)\n",
    "                correct += (pred == labels).cpu().sum()\n",
    "                total += labels.size(0)\n",
    "            total = len(testloader.dataset)\n",
    "            accuracy = 100.0*correct/total\n",
    "        '''\n",
    "        Save loss\n",
    "        '''\n",
    "#         scheduler.step()\n",
    "        lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        trainloss_list.append(train_loss)\n",
    "        testloss_list.append(test_loss)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('{}/{} Test set: Average loss: {:.4f}/{:.4f}, Accuracy: {}/{} ({:.2f}%) lr={}'.format(\n",
    "                epoch, Epochs, train_loss,test_loss, correct, total, accuracy, lr_list[-1]))\n",
    "\n",
    "    return [Model,\n",
    "            trainloss_list,\n",
    "            testloss_list,\n",
    "            accuracy_list,\n",
    "            lr_list,\n",
    "            F]\n",
    "\n",
    "# [_,trainloss_1,testloss_1,accuracy_1,_,F_1] = train_MNIST(model_name=DNN_MNIST_N(28*28,10,20,10,10),Batch=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d3ae88",
   "metadata": {},
   "source": [
    "### Batch 64 vs 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f57355",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Model_64,trainloss_64,testloss_64,accuracy_64,lr_64,_] = train_MNIST(model_name=DNN_MNIST_N(28*28,100,200,100,10),Batch=64)\n",
    "[Model_2048,trainloss_2048,testloss_2048,accuracy_2048,lr_2048,_] = train_MNIST(model_name=DNN_MNIST_N(28*28,100,200,100,10),Batch=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd847709",
   "metadata": {},
   "source": [
    "### lr 0.01 vs 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Model_1e3,trainloss_1e3,testloss_1e3,accuracy_1e3,lr_1e3,_] = train_MNIST(model_name=DNN_MNIST_N(28*28,100,200,100,10),LR=0.001)\n",
    "[Model_1e2,trainloss_1e2,testloss_1e2,accuracy_1e2,lr_1e2,_] = train_MNIST(model_name=DNN_MNIST_N(28*28,100,200,100,10),LR=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plot loss & acc\n",
    "'''\n",
    "plt.figure()\n",
    "plt.plot(trainloss_64, label='Model_64')\n",
    "plt.plot(trainloss_2048, label='Model_2048')\n",
    "plt.xlabel('epoch',fontsize=20)\n",
    "plt.ylabel('loss',fontsize=20)\n",
    "plt.title('Train loss',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(testloss_64, label='Model_64')\n",
    "plt.plot(testloss_2048, label='Model_2048')\n",
    "plt.xlabel('epoch',fontsize=20)\n",
    "plt.ylabel('loss',fontsize=20)\n",
    "plt.title('Test loss',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4597507",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data/',train=True,download=True,transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.MNIST(root='./data/',train=False,download=True,transform=transforms.ToTensor())\n",
    "trainloader = DataLoader(trainset, batch_size=2000, shuffle=True, num_workers=0)\n",
    "testloader  = DataLoader(testset,  batch_size=2000, shuffle=True, num_workers=0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "Model = DNN_MNIST_N(28*28,100,200,100,10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "param_1 = Model_64.state_dict()\n",
    "param_2 = Model_2048.state_dict()\n",
    "\n",
    "batch_train_loss = []\n",
    "batch_test_loss = []\n",
    "batch_train_acc = []\n",
    "batch_test_acc = []\n",
    "\n",
    "alpha_list = np.linspace(-2,2,50)\n",
    "for i in range(len(alpha_list)):\n",
    "    alpha = alpha_list[i]\n",
    "    param_new = {}\n",
    "    for key in param_1.keys():\n",
    "        param_new[key] = (1-alpha)*param_1[key] + alpha*param_2[key]\n",
    "    Model.load_state_dict(param_new)\n",
    "    Model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "            images = (images.view(-1, 28*28)).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = Model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            correct += (pred == labels).cpu().sum()\n",
    "        total = len(trainloader.dataset)\n",
    "        accuracy = 100.0*correct/total\n",
    "        batch_train_loss.append(loss.detach().cpu().numpy())\n",
    "        batch_train_acc.append(accuracy.detach().cpu().numpy())\n",
    "        \n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = (images.view(-1, 28*28)).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = Model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            correct += (pred == labels).cpu().sum()\n",
    "        total = len(testloader.dataset)\n",
    "        accuracy = 100.0*correct/total\n",
    "        batch_test_loss.append(loss.detach().cpu().numpy())\n",
    "        batch_test_acc.append(accuracy.detach().cpu().numpy())\n",
    "\n",
    "    print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data/',train=True,download=True,transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.MNIST(root='./data/',train=False,download=True,transform=transforms.ToTensor())\n",
    "trainloader = DataLoader(trainset, batch_size=2000, shuffle=True, num_workers=0)\n",
    "testloader  = DataLoader(testset,  batch_size=2000, shuffle=True, num_workers=0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "Model = DNN_MNIST_N(28*28,100,200,100,10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "param_1 = Model_1e3.state_dict()\n",
    "param_2 = Model_1e2.state_dict()\n",
    "\n",
    "lr_train_loss = []\n",
    "lr_train_acc = []\n",
    "lr_test_loss = []\n",
    "lr_test_acc = []\n",
    "\n",
    "alpha_list = np.linspace(0,1,50)\n",
    "for i in range(len(alpha_list)):\n",
    "    alpha = alpha_list[i]\n",
    "    param_new = {}\n",
    "    for key in param_1.keys():\n",
    "        param_new[key] = (1-alpha)*param_1[key] + alpha*param_2[key]\n",
    "    Model.load_state_dict(param_new)\n",
    "    Model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "            images = (images.view(-1, 28*28)).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = Model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            correct += (pred == labels).cpu().sum()\n",
    "        total = len(trainloader.dataset)\n",
    "        accuracy = 100.0*correct/total\n",
    "        lr_train_loss.append(loss.detach().cpu().numpy())\n",
    "        lr_train_acc.append(accuracy.detach().cpu().numpy())\n",
    "        \n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = (images.view(-1, 28*28)).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = Model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            correct += (pred == labels).cpu().sum()\n",
    "        total = len(testloader.dataset)\n",
    "        accuracy = 100.0*correct/total\n",
    "        lr_test_loss.append(loss.detach().cpu().numpy())\n",
    "        lr_test_acc.append(accuracy.detach().cpu().numpy())\n",
    "\n",
    "    print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ea05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(alpha_list,standardization(batch_train_loss), label='train_loss')\n",
    "plt.plot(alpha_list,standardization(batch_test_loss),label='test_loss')\n",
    "plt.plot(alpha_list,standardization(batch_train_acc),label='train_accuracy')\n",
    "plt.plot(alpha_list,standardization(batch_test_acc),label='test_accuracy')\n",
    "plt.xlabel('alpha',fontsize=20)\n",
    "plt.ylabel('loss',fontsize=20)\n",
    "plt.title('Batch Size',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(alpha_list,(lr_train_loss), label='train_loss')\n",
    "plt.plot(alpha_list,(lr_test_loss), label='test_loss')\n",
    "plt.plot(alpha_list,standardization(lr_train_acc), label='train_accuracy')\n",
    "plt.plot(alpha_list,standardization(lr_test_acc), label='test_accuracy')\n",
    "plt.xlabel('alpha',fontsize=20)\n",
    "plt.ylabel('loss',fontsize=20)\n",
    "plt.title('Learning Rate',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd9c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "618b928f",
   "metadata": {},
   "source": [
    "## Flatness v.s. Generalization - part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3dadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[_,trainloss_1,testloss_1,accuracy_1,_,F_1] = train_MNIST(model_name=DNN_MNIST_N(28*28,10,20,10,10),Batch=10)\n",
    "[_,trainloss_2,testloss_2,accuracy_2,_,F_2] = train_MNIST(model_name=DNN_MNIST_N(28*28,10,20,10,10),Batch=50)\n",
    "[_,trainloss_3,testloss_3,accuracy_3,_,F_3] = train_MNIST(model_name=DNN_MNIST_N(28*28,10,20,10,10),Batch=100)\n",
    "[_,trainloss_4,testloss_4,accuracy_4,_,F_4] = train_MNIST(model_name=DNN_MNIST_N(28*28,10,20,10,10),Batch=200)\n",
    "[_,trainloss_5,testloss_5,accuracy_5,_,F_5] = train_MNIST(model_name=DNN_MNIST_N(28*28,10,20,10,10),Batch=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b610b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37dfd60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
